import json, requests, time, os, re, base58
from concurrent.futures import ThreadPoolExecutor

# æåº¦æ‰©å¼ çš„æ¥å£æ± 
POOL_URLS = [
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/0827.json",
    "https://itvbox.top/tv",
    "http://cdn.qiaoji8.com/tvbox.json",
    "http://bbk.888tv.tv/itvbox.json",
    "http://meitv.top/itvbox.json",
    "http://120.79.4.185/new.json",
    "https://ghproxy.com/https://raw.githubusercontent.com/ssili126/tv/main/itvbox.json",
    "https://raw.githubusercontent.com/FongMi/Release/main/levon.json",
    "http://home.jundie.top:81/top98.json",
    "https://t-v.me/tv.json",
    "http://pandown.pro/tvbox/m.json"
]

def check_source(name, api):
    try:
        # å®½å®¹æµ‹é€Ÿï¼š15ç§’ã€‚é’ˆå¯¹è“å…‰æºä¼˜åŒ–
        res = requests.get(api, timeout=15, headers={'User-Agent': 'Mozilla/5.0'}, verify=False)
        if res.status_code == 200:
            return {"api": api, "name": name, "delay": res.elapsed.total_seconds()}
    except:
        pass
    return None

def generate():
    raw_links = set()
    print("æ­£åœ¨æœåˆ®å…¨ç½‘èµ„æºï¼Œè¯·ç¨å...")
    
    for url in POOL_URLS:
        try:
            r = requests.get(url, timeout=10, verify=False)
            # å…¼å®¹æ›´å¤šæ ¼å¼çš„æ­£åˆ™
            found = re.findall(r'"(https?://[^"]+/api\.php/provide/vod[^"]*)"', r.text)
            for link in found:
                raw_links.add(link)
        except:
            continue
    
    print(f"æ‰¾åˆ°æ½œåœ¨æ¥å£ {len(raw_links)} ä¸ªï¼Œå¼€å§‹ç­›é€‰æœ‰æ•ˆæº...")

    valid_results = []
    with ThreadPoolExecutor(max_workers=50) as exe:
        futures = [exe.submit(check_source, f"æº_{i}", url) for i, url in enumerate(list(raw_links))]
        for f in futures:
            res = f.result()
            if res:
                valid_results.append(res)
    
    valid_results.sort(key=lambda x: x['delay'])
    final_list = valid_results[:80] 

    api_site = {}
    for i, item in enumerate(final_list):
        key = f"api_{i+1}"
        api_site[key] = {
            "api": item['api'],
            "name": f"ğŸš€ æº_{i+1:02d} | {int(item['delay']*1000)}ms",
            "detail": item['api'].split('api.php')[0]
        }

    # ç¬¦åˆ DecoTV ä¸“ç”¨çš„åµŒå¥— JSON æ ¼å¼
    config = {
        "cache_time": 9200,
        "api_site": api_site,
        "custom_category": [
            { "name": "åè¯­", "type": "movie", "query": "åè¯­" },
            { "name": "4Ké‡å‹", "type": "movie", "query": "4K" }
        ]
    }

    # 1. ä¿å­˜ä¸ºæ™®é€šçš„ JSON æ–‡ä»¶
    with open("tv.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # 2. æ ¸å¿ƒä¿®æ”¹ï¼šå°†åˆšæ‰ç”Ÿæˆçš„ config ç¼–ç ä¸º Base58
    compact_json = json.dumps(config, ensure_ascii=False).encode('utf-8')
    b58_encoded_text = base58.b58encode(compact_json).decode('utf-8')

    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(b58_encoded_text)

    print(f"âœ… ç”Ÿæˆå®Œæˆï¼")
    print(f"- tv.json (åŒ…å« {len(api_site)} ä¸ªæº)")
    print(f"- deco_b58.txt (å·²å®Œæˆ Base58 ç¼–ç )")

if __name__ == "__main__":
    # ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
    requests.packages.urllib3.disable_warnings()
    generate()
