import json, requests, time, os, re, base58
from concurrent.futures import ThreadPoolExecutor

# 1. æåº¦æ‰©å¼ çš„æ¥å£æ± 
POOL_URLS = [
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/0827.json",
    "https://itvbox.top/tv",
    "http://cdn.qiaoji8.com/tvbox.json",
    "http://bbk.888tv.tv/itvbox.json",
    "http://meitv.top/itvbox.json",
    "http://120.79.4.185/new.json",
    "https://ghproxy.com/https://raw.githubusercontent.com/ssili126/tv/main/itvbox.json",
    "https://raw.githubusercontent.com/FongMi/Release/main/levon.json",
    "http://home.jundie.top:81/top98.json",
    "https://t-v.me/tv.json",
    "http://pandown.pro/tvbox/m.json"
]

def check_source(name, api):
    """æ£€æµ‹æºæ˜¯å¦é€šç•…"""
    try:
        # é’ˆå¯¹ 115/è“å…‰æºï¼Œæ”¾å®½åˆ° 15 ç§’è¶…æ—¶
        res = requests.get(api, timeout=15, headers={'User-Agent': 'Mozilla/5.0'}, verify=False)
        if res.status_code == 200:
            return {"api": api, "name": name, "delay": res.elapsed.total_seconds()}
    except:
        pass
    return None

def generate():
    """ä¸»ç”Ÿæˆå‡½æ•°"""
    raw_links = set()
    print("å¼€å§‹ä»å…¨ç½‘èšåˆåœ°å€æŠ“å–æ¥å£...")
    
    for url in POOL_URLS:
        try:
            r = requests.get(url, timeout=10, verify=False)
            # æå–ç¬¦åˆ CMS æ¥å£ç‰¹å¾çš„é“¾æ¥
            found = re.findall(r'"(https?://[^"]+/api\.php/provide/vod[^"]*)"', r.text)
            for link in found:
                raw_links.add(link)
        except:
            continue
    
    print(f"æœåˆ®åˆ° {len(raw_links)} ä¸ªæ½œåœ¨æ¥å£ï¼Œæ­£åœ¨å¹¶è¡Œæµ‹é€Ÿ...")

    valid_results = []
    # ä½¿ç”¨ 50 çº¿ç¨‹å¹¶è¡Œæ£€æµ‹
    with ThreadPoolExecutor(max_workers=50) as exe:
        futures = [exe.submit(check_source, f"æº_{i}", url) for i, url in enumerate(list(raw_links))]
        for f in futures:
            res = f.result()
            if res:
                valid_results.append(res)
    
    # æ’åºï¼šæŒ‰å»¶è¿Ÿæ’åº
    valid_results.sort(key=lambda x: x['delay'])
    # å–å‰ 80 ä¸ªæœ€ç¨³çš„ç«™
    final_list = valid_results[:80] 

    api_site = {}
    for i, item in enumerate(final_list):
        key = f"api_{i+1}"
        api_site[key] = {
            "api": item['api'],
            "name": f"ğŸš€ æº_{i+1:02d} | {int(item['delay']*1000)}ms",
            "detail": item['api'].split('api.php')[0]
        }

    # æ„é€ ç¬¦åˆ DecoTV ä¸“ç”¨æ ¼å¼çš„é…ç½®å¯¹è±¡
    config = {
        "cache_time": 9200,
        "api_site": api_site,
        "custom_category": [
            { "name": "åè¯­", "type": "movie", "query": "åè¯­" },
            { "name": "4Ké‡å‹", "type": "movie", "query": "4K" }
        ]
    }

    # --- è¾“å‡ºæµç¨‹ ---

    # 1. ä¿å­˜åŸå§‹ JSON
    with open("tv.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²ç”Ÿæˆ tv.json (åŒ…å« {len(api_site)} ä¸ªæº)")
    
    # 2. æ ¸å¿ƒï¼šå°† config å­—å…¸è½¬æ¢ä¸º Base58 ç¼–ç 
    # å…ˆåºåˆ—åŒ–æˆç´§å‡‘çš„å­—ç¬¦ä¸²ï¼Œå†ç¼–ç 
    json_bytes = json.dumps(config, ensure_ascii=False).encode('utf-8')
    b58_text = base58.b58encode(json_bytes).decode('utf-8')

    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(b58_text)
    print("âœ… å·²ç”Ÿæˆ deco_b58.txt (Base58 ç¼–ç å®Œæˆ)")

if __name__ == "__main__":
    # ç¦ç”¨ HTTPS è­¦å‘Šï¼ˆé˜²æ­¢è„šæœ¬å› æŸäº›æºè¯ä¹¦é—®é¢˜ä¸­æ–­ï¼‰
    try:
        requests.packages.urllib3.disable_warnings()
    except:
        pass
    generate()
